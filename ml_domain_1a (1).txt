# -*- coding: utf-8 -*-
"""ML_DOMAIN_1A.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14xgOyXnF_tUbuOImiAanmLqOXkjqXghA
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader, random_split
import matplotlib.pyplot as plt
import os
import pickle
import pandas as pd

!pip install -q kaggle
from google.colab import files
files.upload()
! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json
! kaggle datasets list
!kaggle datasets download -d crawford/emnist
!unzip -q emnist.zip
!ls -F /content/emnist



train_dir = "/content/emnist-balanced-train.csv"
test_dir = "/content/emnist-balanced-test.csv"

train_dataset = pd.read_csv(train_dir)
test_dataset = pd.read_csv(test_dir)

#print(train_dataset.head())
#print(train_dataset.shape)
#print(test_dataset.shape)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_dataset = DataLoader(val_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)

#Create a CNN module for model
class CNNmodel(nn.Module):
  def __init__(self):
    super(CNNmodel, self).__init__()

    self.conv1 = nn.Sequential(nn.Flatten(),
    nn.Linear(784,16),nn.ReLU())

    self.left_1 = nn.Sequential(nn.Linear(16,8),
                  nn.ReLU())
    self.left_2 = nn.Sequential(nn.Linear(8,8),
                  nn.ReLU())
    self.right = nn.Sequential(
        nn.Linear(16,12),
        nn.ReLU(),
        nn.Linear(12,8),
        nn.ReLU()
    )
  def forward(self, x):
    x=self.conv1(x)
    l1 = self.left_1(x)
    l2 = self.left_2(x)
    l = l1 + l2
    x = torch.cat((l, self.right(x)), dim=1)
    return x

model = CNNmodel().to('cuda'if torch.cuda.is_available() else 'cpu')
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

epoch = 5
train_loss = []
train_acc = []
val_loss = []
val_acc = []
for i in range(epoch):
  model.train()
  running_loss,accuracy,total = 0.0,0.0,0
  for batch,(x, y) in enumerate(train_loader):
    x, y = x.to('cuda'if torch.cuda.is_available() else 'cpu'), y.to('cuda'if torch.cuda.is_available() else 'cpu')
    pred = model(x)
    loss = loss_fn(pred, y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    running_loss += loss.item()
    _, predicted = torch.max(pred.data, 1)
    total += y.size(0)
    accuracy += (predicted == y).sum().item()
  train_loss.append(running_loss / len(train_loader))
  train_acc.append(accuracy / total)

  model.eval()
  running_loss,accuracy,total = 0.0,0.0,0
  with torch.inference():
    for batch, (x, y) in enumerate(val_loader):
      x, y = x.to('cuda'if torch.cuda.is_available() else 'cpu'), y.to('cuda'if torch.cuda.is_available() else 'cpu')
      pred = model(x)
      loss = loss_fn(pred, y)
      running_loss += loss.item()
      _, predicted = torch.max(pred.data, 1)
      total += y.size(0)
      accuracy += (predicted == y).sum().item()
    val_loss.append(running_loss / len(val_loader))
    val_acc.append(accuracy / total)
  print(f"Train Loss  : {train_loss}")

plt.figure(figsize=(20,20))

plt.subplot(1,2,1)
plt.plot(train_loss, label='train_loss')
plt.plot(val_loss, label='val_loss')
plt.title('LOSS vs EPOCH')
plt.legend()

plt.subplot(1,2,2)
plt.plot(train_acc, label='train_acc')
plt.plot(val_acc, label='val_acc')
PLT.title('ACC vs EPOCH')
plt.legend()

plt.show()

model.eval()
with torch.inference():
  pred_list=[]
  for batch, (x, y) in enumerate(test_loader):
    x,y = x.to('cuda'if torch.cuda.is_available() else 'cpu'), y.to('cuda'if torch.cuda.is_available() else 'cpu')
    pred = model(x)
    loss = loss_fn(pred, y)
    _, predicted = torch.max(pred.data, 1)
    pred_list.extend(predicted.cpu().numpy())

df = pd.DataFrame({'Input':np.arange(len(pred_list)),'prediction': pred_list})
df.to_csv('submission.csv', index=False)